from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_core.chat_history import BaseChatMessageHistory
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain.chains import create_history_aware_retriever, create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
#from langchain.chat_models import ChatOpenAI
from langchain_community.chat_models import ChatOpenAI
from langchain_upstage import UpstageEmbeddings
from langchain_chroma import Chroma
from sentence_transformers import CrossEncoder
from langchain_core.retrievers import BaseRetriever
from langchain_core.documents import Document
from pydantic import BaseModel, PrivateAttr
from typing import List
from functools import lru_cache
import streamlit as st
import os
from dotenv import load_dotenv
import traceback

# ======================== ì„¤ì • ========================
load_dotenv(dotenv_path=".envfile", override=True)
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
UPSTAGE_API_KEY = os.getenv("UPSTAGE_API_KEY")
os.environ["OPENAI_API_KEY"] = OPENAI_API_KEY
EMBEDDING_MODEL = "solar-embedding-1-large"
PERSIST_DIR = "./chroma_rag_demo"
COLLECTION_NAME = "rag-demo"

# ======================== ì „ì—­ ì €ì¥ì†Œ ========================
store = {}

# ======================== ì „ì—­ í”„ë¡¬í”„íŠ¸ ========================
SYSTEM_PROMPT = (
    "ë‹¹ì‹ ì€ ë³´í—˜ ìƒë‹´ì‚¬ì˜ ì—­í• ì„ ìˆ˜í–‰í•˜ëŠ” AIì…ë‹ˆë‹¤.\n\n"
    "ì•„ë˜ëŠ” ë³´í—˜ ìƒë‹´ì‚¬ê°€ ê³ ê°ë‹˜ê³¼ ì „í™” ìƒë‹´ì„ ì§„í–‰í•  ë•Œ ì°¸ê³ í•˜ëŠ” ë¬¸ì„œì…ë‹ˆë‹¤. "
    "ì´ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ, ìƒë‹´ì›ì´ ê³ ê°ë‹˜ê»˜ ì§ì ‘ ì „í™”ë¡œ ì•ˆë‚´í•˜ë“¯ì´ ìì—°ìŠ¤ëŸ½ê³  ì¹œì ˆí•œ ë§íˆ¬ë¡œ ì‘ë‹µí•´ ì£¼ì„¸ìš”.\n\n"
    "[ë¬¸ì„œ ìš”ì•½ ì •ë³´]\n{context}\n\n"
    "[ê³ ê° ì§ˆë¬¸]\n{input}\n\n"
    "[ì‘ë‹µ êµ¬ì„± ì§€ì¹¨]\n"
    "1. ë‹µë³€ì€ ë‹¤ìŒ ë‘ ê°€ì§€ í˜•ì‹ìœ¼ë¡œ ëª¨ë‘ êµ¬ì„±í•´ ì£¼ì„¸ìš”: ë‘ í˜•ì‹ ì‚¬ì´ì—ëŠ” êµ¬ë¶„ì„ ì„ ë„£ì–´ì£¼ì„¸ìš”.\n"
    "   - ì²«ì§¸, ìƒë‹´ì›ì´ ê³ ê°ë‹˜ê³¼ í†µí™”í•˜ë©° ê·¸ëŒ€ë¡œ ì½ì„ ìˆ˜ ìˆëŠ” **ìŠ¤í¬ë¦½íŠ¸ í˜•ì‹**ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.\n"
    "   - ë‘˜ì§¸, ê³ ê°ì´ í™”ë©´ìƒì—ì„œ ì‰½ê²Œ ì´í•´í•  ìˆ˜ ìˆë„ë¡ **í•­ëª©ë³„ ì •ë¦¬ëœ ì •ë³´ ë¸”ë¡ (ë§ˆí¬ë‹¤ìš´)**ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.\n"
    "2. ìŠ¤í¬ë¦½íŠ¸ëŠ” â€˜ì•ˆë…•í•˜ì„¸ìš” ê³ ê°ë‹˜~â€™, â€˜ë„ì›€ì´ ë˜ì…¨ìœ¼ë©´ ì¢‹ê² ìŠµë‹ˆë‹¤~â€™ ê°™ì€ ìì—°ìŠ¤ëŸ¬ìš´ ë§íˆ¬ë¡œ ë§í•´ì£¼ì„¸ìš”.\n"
    "3. ì •ë³´ ë¸”ë¡ì€ ì†Œì œëª©, ëª©ë¡, ê°•ì¡°, ì¤„ë°”ê¿ˆ ë“±ì„ ì ì ˆíˆ ì‚¬ìš©í•´ ë³´ê¸° ì‰½ê²Œ êµ¬ì„±í•´ ì£¼ì„¸ìš”.\n"
    "4. ì†Œì œëª©ì€ ë„ˆë¬´ í¬ì§€ ì•Šê²Œ êµµì€ ê¸€ì”¨ë‚˜ ë°•ìŠ¤ í˜•íƒœë¡œ, Streamlitì—ì„œë„ ì˜ ë³´ì´ë„ë¡ ì‘ì„±í•´ ì£¼ì„¸ìš”.\n"
    "5. ê¸´ ì„¤ëª…ì€ ë‚˜ì—´í•˜ì§€ ë§ê³ , í•­ëª©ë³„ë¡œ ìš”ì•½ ì •ë¦¬í•´ ì£¼ì„¸ìš”.\n"
    "6. ë¬¸ì„œì—ì„œ í™•ì¸í•  ìˆ˜ ì—†ëŠ” ë‚´ìš©ì€ í™•ì‹ í•˜ì§€ ë§ê³ , ì¶”ê°€ í™•ì¸ì´ í•„ìš”í•˜ë‹¤ê³  ì•ˆë‚´í•´ ì£¼ì„¸ìš”.\n"
    "7. ëª¨ë“  ì‘ë‹µì€ ë°˜ë“œì‹œ ìƒë‹´ìš© êµ¬ì–´ì²´ë¡œ ì‘ì„±í•´ ì£¼ì„¸ìš”.\n\n"
    "[ì‘ë‹µ ì˜ˆì‹œ í˜•ì‹]\n"
    "â–¶ï¸ ìš”ì•½ ì •ë³´:\n"
    "ë§ˆí¬ë‹¤ìš´ ëª©ë¡ í˜•ì‹ìœ¼ë¡œ ì•Œì•„ë³´ê¸° ì‰½ê²Œ ì •ë¦¬"
    "â–¶ï¸ ìƒë‹´ ìŠ¤í¬ë¦½íŠ¸:\n"
    "ì•ˆë…•í•˜ì„¸ìš”, ê³ ê°ë‹˜! ğŸ˜Š ë¬¸ì˜í•˜ì‹  ë‹¤ìë…€ í• ì¸ í˜œíƒì— ëŒ€í•´ ì•ˆë‚´ë“œë¦´ê²Œìš”. ...\n\n"
)

# ======================== ìºì‹œëœ í•¨ìˆ˜ ========================
@lru_cache(maxsize=1)
def get_llm(model='gpt-4o-mini'):
    return ChatOpenAI(model=model)

@st.cache_resource
def get_vectorstore():
    embedding = UpstageEmbeddings(api_key=UPSTAGE_API_KEY, model=EMBEDDING_MODEL)
    return Chroma(collection_name=COLLECTION_NAME, persist_directory=PERSIST_DIR, embedding_function=embedding)

@st.cache_resource
def get_reranker():
    return CrossEncoder("cross-encoder/ms-marco-MiniLM-L-6-v2")

# ======================== Re-ranking ì •ì˜ ========================
def rerank_documents(query, docs, top_k=5):
    pairs = [(query, doc.page_content) for doc in docs]
    scores = get_reranker().predict(pairs)
    reranked = sorted(zip(docs, scores), key=lambda x: x[1], reverse=True)
    return [doc for doc, _ in reranked[:top_k]]

class RerankRetriever(BaseRetriever):
    def __init__(self, base_retriever, top_k=5, **kwargs):
        super().__init__(**kwargs)
        self._base_retriever = base_retriever
        self._top_k = top_k

    def get_relevant_documents(self, query: str) -> List[Document]:
        docs = self._base_retriever.get_relevant_documents(query)
        return rerank_documents(query, docs, top_k=self._top_k)

    async def aget_relevant_documents(self, query: str) -> List[Document]:
        docs = await self._base_retriever.aget_relevant_documents(query)
        return rerank_documents(query, docs, top_k=self._top_k)

# ======================== ì„¸ì…˜ ê´€ë¦¬ ========================
def get_session_history(session_id: str) -> BaseChatMessageHistory:
    if session_id not in store:
        store[session_id] = ChatMessageHistory()
    return store[session_id]

# ======================== ì²´ì¸ êµ¬ì„± ========================
def get_dictionary_chain():
    dictionary = ["BEFORE -> AFTER"]
    prompt = ChatPromptTemplate.from_template(f"""
        ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë³´ê³ , ìš°ë¦¬ì˜ ì‚¬ì „ì„ ì°¸ê³ í•´ì„œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë³€ê²½í•´ì£¼ì„¸ìš”.
        ë§Œì•½ ë³€ê²½í•  í•„ìš”ê°€ ì—†ë‹¤ê³  íŒë‹¨ëœë‹¤ë©´, ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë³€ê²½í•˜ì§€ ì•Šì•„ë„ ë©ë‹ˆë‹¤.
        ê·¸ëŸ° ê²½ìš°ì—ëŠ” ì§ˆë¬¸ë§Œ ë¦¬í„´í•´ì£¼ì„¸ìš”
        ì‚¬ì „: {dictionary}

        ì§ˆë¬¸: {{input}}
    """)
    return prompt | get_llm() | StrOutputParser()

def get_rerank_retriever():
    base_retriever = get_vectorstore().as_retriever(search_kwargs={"k": 10})
    return RerankRetriever(base_retriever=base_retriever, top_k=5)

def get_history_retriever():
    contextualize_q_prompt = ChatPromptTemplate.from_messages([
        ("system", "Given a chat history and the latest user question "
                    "which might reference context in the chat history, "
                    "formulate a standalone question which can be understood "
                    "without the chat history. Do NOT answer the question, "
                    "just reformulate it if needed and otherwise return it as is."),
        MessagesPlaceholder("chat_history"),
        ("human", "{input}"),
    ])
    return create_history_aware_retriever(get_llm(), get_rerank_retriever(), contextualize_q_prompt)

def get_rag_chain():
    qa_prompt = ChatPromptTemplate.from_messages([
        ("system", SYSTEM_PROMPT),
        MessagesPlaceholder("chat_history"),
        ("human", "{input}"),
    ])

    retriever = get_history_retriever()
    rag_chain = create_retrieval_chain(retriever, create_stuff_documents_chain(get_llm(), qa_prompt))

    return RunnableWithMessageHistory(
        rag_chain,
        get_session_history,
        input_messages_key="input",
        history_messages_key="chat_history",
        output_messages_key="answer",
    ).pick('answer')

def get_ai_response(user_message, session_id="abc123"):
    try:
        dictionary_chain = get_dictionary_chain()
        rag_chain = get_rag_chain()
        rag_pipeline = {"input": dictionary_chain} | rag_chain

        result = rag_pipeline.invoke(
            {"input": user_message},
            config={"configurable": {"session_id": session_id}}
        )
        return iter([result])
    except Exception as e:
        import streamlit as st
        st.error("ğŸ”¥ invoke ì¤‘ ì˜ˆì™¸ ë°œìƒ! ì½˜ì†” ë¡œê·¸ë„ í™•ì¸í•´ì£¼ì„¸ìš”.")
        print("ğŸ”¥ invoke ì‹¤í–‰ ì¤‘ ì˜ˆì™¸ ë°œìƒ:", e)
        traceback.print_exc()  # ğŸ”¥ ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤ ì „ì²´ ì¶œë ¥
        return iter(["âŒ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•´ ì£¼ì„¸ìš”."])

